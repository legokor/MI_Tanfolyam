{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hPQ75HzFaEjx"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import MultivariateNormal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "    self.layer1 = nn.Linear(in_dim, 64)\n",
        "    self.layer2 = nn.Linear(64, 64)\n",
        "    self.layer3 = nn.Linear(64, out_dim)\n",
        "\n",
        "  def forward(self, obs):\n",
        "    # Convert observation to tensor if it's a numpy array\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = torch.tensor(obs, dtype=torch.float)\n",
        "\n",
        "    activation1 = F.relu(self.layer1(obs))\n",
        "    activation2 = F.relu(self.layer2(activation1))\n",
        "    output = self.layer3(activation2)\n",
        "    return output"
      ],
      "metadata": {
        "id": "EjVO0wvjanml"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "  def __init__(self, policy_class, env, **hyperparameters):\n",
        "    # Make sure the environment is compatible with our code\n",
        "    assert(type(env.observation_space) == gym.spaces.Box)\n",
        "    assert(type(env.action_space) == gym.spaces.Box)\n",
        "\n",
        "    # Initialize hyperparameters for training with PPO\n",
        "    self._init_hyperparameters(hyperparameters)\n",
        "\n",
        "    # Extract environment information\n",
        "    self.env = env\n",
        "    self.obs_dim = env.observation_space.shape[0]\n",
        "    self.act_dim = env.action_space.shape[0]\n",
        "\n",
        "    # Initialize actor and critic networks\n",
        "    self.actor = FeedForwardNN(self.obs_dim, self.act_dim)# ALG STEP 1\n",
        "    self.critic = FeedForwardNN(self.obs_dim, 1)\n",
        "\n",
        "    # Initialize optimizers for actor and critic\n",
        "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
        "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "    # Initialize the covariance matrix used to query the actor for actions\n",
        "    self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
        "    self.cov_mat = torch.diag(self.cov_var)\n",
        "\n",
        "    # This logger will help us with printing out summaries of each iteration\n",
        "    self.logger = {\n",
        "\t\t\t'delta_t': time.time_ns(),\n",
        "\t\t\t't_so_far': 0,          # timesteps so far\n",
        "\t\t\t'i_so_far': 0,          # iterations so far\n",
        "\t\t\t'batch_lens': [],       # episodic lengths in batch\n",
        "\t\t\t'batch_rews': [],       # episodic returns in batch\n",
        "\t\t\t'actor_losses': [],     # losses of actor network in current iteration\n",
        "\t\t}\n",
        "\n",
        "  \"\"\"\n",
        "\t\t\tTrain the actor and critic networks. Here is where the main PPO algorithm resides.\n",
        "\t\t\tParameters:\n",
        "\t\t\t\ttotal_timesteps - the total number of timesteps to train for\n",
        "\t\t\tReturn:\n",
        "\t\t\t\tNone\n",
        "\t\"\"\"\n",
        "  def learn(self, total_timesteps):\n",
        "    print(f\"Learning... Running {self.max_timesteps_per_episode} timesteps per episode, \", end='')\n",
        "    print(f\"{self.timesteps_per_batch} timesteps per batch for a total of {total_timesteps} timesteps\")\n",
        "    t_so_far = 0 # Timesteps simulated so far\n",
        "    i_so_far = 0 # Iterations ran so far\n",
        "    while t_so_far < total_timesteps:                                                                       # ALG STEP 2\n",
        "\t\t\t#Collecting our batch simulations here\n",
        "      batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()                     # ALG STEP 3\n",
        "\n",
        "\t\t\t# Calculate how many timesteps we collected this batch\n",
        "      t_so_far += np.sum(batch_lens)\n",
        "\n",
        "\t\t\t# Increment the number of iterations\n",
        "      i_so_far += 1\n",
        "\n",
        "\t\t\t# Logging timesteps so far and iterations so far\n",
        "      self.logger['t_so_far'] = t_so_far\n",
        "      self.logger['i_so_far'] = i_so_far\n",
        "\n",
        "\t\t\t# Calculate advantage at k-th iteration\n",
        "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "      A_k = batch_rtgs - V.detach()                                                                       # ALG STEP 5\n",
        "\n",
        "\t\t\t# One of the only tricks I use that isn't in the pseudocode. Normalizing advantages\n",
        "\t\t\t# isn't theoretically necessary, but in practice it decreases the variance of\n",
        "\t\t\t# our advantages and makes convergence much more stable and faster. I added this because\n",
        "\t\t\t# solving some environments was too unstable without it.\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
        "\n",
        "\t\t\t# This is the loop where we update our network for some n epochs\n",
        "      for _ in range(self.n_updates_per_iteration):                                                       # ALG STEP 6 & 7\n",
        "\t\t\t\t# Calculate V_phi and pi_theta(a_t | s_t)\n",
        "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "\n",
        "\t\t\t\t# Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n",
        "\t\t\t\t# NOTE: we just subtract the logs, which is the same as\n",
        "\t\t\t\t# dividing the values and then canceling the log with e^log.\n",
        "\t\t\t\t# For why we use log probabilities instead of actual probabilities,\n",
        "\t\t\t\t# here's a great explanation:\n",
        "\t\t\t\t# https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
        "\t\t\t\t# TL;DR makes gradient ascent easier behind the scenes.\n",
        "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
        "\n",
        "\t\t\t\t# Calculate surrogate losses.\n",
        "        surr1 = ratios * A_k\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
        "\n",
        "\t\t\t\t# Calculate actor and critic losses.\n",
        "\t\t\t\t# NOTE: we take the negative min of the surrogate losses because we're trying to maximize\n",
        "\t\t\t\t# the performance function, but Adam minimizes the loss. So minimizing the negative\n",
        "\t\t\t\t# performance function maximizes it.\n",
        "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
        "        critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
        "\n",
        "\t\t\t\t# Calculate gradients and perform backward propagation for actor network\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward(retain_graph=True)\n",
        "        self.actor_optim.step()\n",
        "\n",
        "\t\t\t\t# Calculate gradients and perform backward propagation for critic network\n",
        "        self.critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "\t\t\t\t# Log actor loss\n",
        "        self.logger['actor_losses'].append(actor_loss.detach())\n",
        "\n",
        "\t\t\t# Print a summary of our training so far\n",
        "      self._log_summary()\n",
        "\n",
        "\t\t\t# Save our model if it's time\n",
        "      if i_so_far % self.save_freq == 0:\n",
        "        torch.save(self.actor.state_dict(), './ppo_actor.pth')\n",
        "        torch.save(self.critic.state_dict(), './ppo_critic.pth')\n",
        "\n",
        "  \"\"\"\n",
        "\t\t\tCollect the batch of data from simulation. Since this is an on-policy algorithm, we'll need to collect a fresh batch\n",
        "\t\t\tof data each time we iterate the actor/critic networks.\n",
        "\t\t\tReturn:\n",
        "\t\t\t\tbatch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
        "\t\t\"\"\"\n",
        "  def rollout(self):\n",
        "\t\t# Batch data.\n",
        "    batch_obs = [] # the observations collected this batch. Shape: (number of timesteps, dimension of observation)\n",
        "    batch_acts = [] # the actions collected this batch. Shape: (number of timesteps, dimension of action)\n",
        "    batch_log_probs = [] # the log probabilities of each action taken this batch. Shape: (number of timesteps)\n",
        "    batch_rews = []    # the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
        "    batch_rtgs = []    # the Rewards-To-Go of each timestep in this batch. Shape: (number of timesteps)\n",
        "    batch_lens = []    # the lengths of each episode this batch. Shape: (number of episodes)\n",
        "\n",
        "\t\t# Episodic data. Keeps track of rewards per episode, will get cleared\n",
        "\t\t# upon each new episode\n",
        "    ep_rews = []\n",
        "\n",
        "    t = 0 # Keeps track of how many timesteps we've run so far this batch\n",
        "\n",
        "\t\t# Keep simulating until we've run more than or equal to specified timesteps per batch\n",
        "    while t < self.timesteps_per_batch:\n",
        "      ep_rews = [] # rewards collected per episode\n",
        "\n",
        "\t\t\t# Reset the environment. sNote that obs is short for observation.\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "\n",
        "\t\t\t# Run an episode for a maximum of max_timesteps_per_episode timesteps\n",
        "      for ep_t in range(self.max_timesteps_per_episode):\n",
        "\t\t\t\t# If render is specified, render the environment\n",
        "        if self.render and (self.logger['i_so_far'] % self.render_every_i == 0) and len(batch_lens) == 0:\n",
        "          self.env.render()\n",
        "\n",
        "        t += 1 # Increment timesteps ran this batch so far\n",
        "\n",
        "\t\t\t\t# Track observations in this batch\n",
        "        batch_obs.append(obs)\n",
        "\n",
        "\t\t\t\t# Calculate action and make a step in the env.\n",
        "\t\t\t\t# Note that rew is short for reward.\n",
        "        action, log_prob = self.get_action(obs)\n",
        "        obs, rew, done, _ = self.env.step(action)\n",
        "\n",
        "\t\t\t\t# Track recent reward, action, and action log probability\n",
        "        ep_rews.append(rew)\n",
        "        batch_acts.append(action)\n",
        "        batch_log_probs.append(log_prob)\n",
        "\n",
        "\t\t\t\t# If the environment tells us the episode is terminated, break\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "\t\t\t# Track episodic lengths and rewards\n",
        "      batch_lens.append(ep_t + 1)\n",
        "      batch_rews.append(ep_rews)\n",
        "\n",
        "\t\t# Reshape data as tensors in the shape specified in function description, before returning\n",
        "    batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
        "    batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
        "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
        "    batch_rtgs = self.compute_rtgs(batch_rews)                                                              # ALG STEP 4\n",
        "\n",
        "\t\t# Log the episodic returns and episodic lengths in this batch.\n",
        "    self.logger['batch_rews'] = batch_rews\n",
        "    self.logger['batch_lens'] = batch_lens\n",
        "\n",
        "    return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
        "\n",
        "  \"\"\"\n",
        "\t\t\tCompute the Reward-To-Go of each timestep in a batch given the rewards.\n",
        "\t\t\tParameters:\n",
        "\t\t\t\tbatch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
        "\t\t\tReturn:\n",
        "\t\t\t\tbatch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
        "\t\t\"\"\"\n",
        "  def compute_rtgs(self, batch_rews):\n",
        "\t\t# The rewards-to-go (rtg) per episode per batch to return.\n",
        "\t\t# The shape will be (num timesteps per episode)\n",
        "    batch_rtgs = []\n",
        "\n",
        "\t\t# Iterate through each episode\n",
        "    for ep_rews in reversed(batch_rews):\n",
        "\n",
        "      discounted_reward = 0 # The discounted reward so far\n",
        "\n",
        "\t\t\t# Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
        "\t\t\t# discounted return (think about why it would be harder starting from the beginning)\n",
        "      for rew in reversed(ep_rews):\n",
        "        discounted_reward = rew + discounted_reward * self.gamma\n",
        "        batch_rtgs.insert(0, discounted_reward)\n",
        "\n",
        "\t\t# Convert the rewards-to-go into a tensor\n",
        "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
        "\n",
        "    return batch_rtgs\n",
        "\n",
        "  \"\"\"\n",
        "\t\t\tQueries an action from the actor network, should be called from rollout.\n",
        "\t\t\tParameters:\n",
        "\t\t\t\tobs - the observation at the current timestep\n",
        "\t\t\tReturn:\n",
        "\t\t\t\taction - the action to take, as a numpy array\n",
        "\t\t\t\tlog_prob - the log probability of the selected action in the distribution\n",
        "\t\"\"\"\n",
        "  def get_action(self, obs):\n",
        "\n",
        "\t\t# Query the actor network for a mean action\n",
        "    mean = self.actor(obs)\n",
        "\n",
        "\t\t# Create a distribution with the mean action and std from the covariance matrix above.\n",
        "\t\t# For more information on how this distribution works, check out Andrew Ng's lecture on it:\n",
        "\t\t# https://www.youtube.com/watch?v=JjB58InuTqM\n",
        "    dist = MultivariateNormal(mean, self.cov_mat)\n",
        "\n",
        "\t\t# Sample an action from the distribution\n",
        "    action = dist.sample()\n",
        "\n",
        "\t\t# Calculate the log probability for that action\n",
        "    log_prob = dist.log_prob(action)\n",
        "\n",
        "\t\t# Return the sampled action and the log probability of that action in our distribution\n",
        "    return action.detach().numpy(), log_prob.detach()\n",
        "\n",
        "  \"\"\"\n",
        "\t\t\tEstimate the values of each observation, and the log probs of each action in the most recent batch with the most recent\n",
        "      iteration of the actor network. Should be called from learn.\n",
        "\t\t\tReturn:\n",
        "\t\t\t\tV - the predicted values of batch_obs\n",
        "\t\t\t\tlog_probs - the log probabilities of the actions taken in batch_acts given batch_obs\n",
        "\t\"\"\"\n",
        "  def evaluate(self, batch_obs, batch_acts):\n",
        "\t\t# Query critic network for a value V for each batch_obs. Shape of V should be same as batch_rtgs\n",
        "    V = self.critic(batch_obs).squeeze()\n",
        "\n",
        "\t\t# Calculate the log probabilities of batch actions using most recent actor network.\n",
        "\t\t# This segment of code is similar to that in get_action()\n",
        "    mean = self.actor(batch_obs)\n",
        "    dist = MultivariateNormal(mean, self.cov_mat)\n",
        "    log_probs = dist.log_prob(batch_acts)\n",
        "\n",
        "\t\t# Return the value vector V of each observation in the batch\n",
        "\t\t# and log probabilities log_probs of each action in the batch\n",
        "    return V, log_probs\n",
        "\n",
        "  \"\"\"\n",
        "\t\tInitialize default and custom values for hyperparameters\n",
        "\t\"\"\"\n",
        "  def _init_hyperparameters(self, hyperparameters):\n",
        "\n",
        "\t\t# Initialize default values for hyperparameters\n",
        "\t\t# Algorithm hyperparameters\n",
        "    self.timesteps_per_batch = 4800                 # Number of timesteps to run per batch\n",
        "    self.max_timesteps_per_episode = 1600           # Max number of timesteps per episode\n",
        "    self.n_updates_per_iteration = 5                # Number of times to update actor/critic per iteration\n",
        "    self.lr = 0.005                                 # Learning rate of actor optimizer\n",
        "    self.gamma = 0.95                               # Discount factor to be applied when calculating Rewards-To-Go\n",
        "    self.clip = 0.2                                 # Recommended 0.2, helps define the threshold to clip the ratio during SGA\n",
        "\n",
        "\t\t# Miscellaneous parameters\n",
        "    self.render = True                              # If we should render during rollout\n",
        "    self.render_every_i = 10                        # Only render every n iterations\n",
        "    self.save_freq = 10                             # How often we save in number of iterations\n",
        "    self.seed = None                                # Sets the seed of our program, used for reproducibility of results\n",
        "\n",
        "\t\t# Change any default values to custom values for specified hyperparameters\n",
        "    for param, val in hyperparameters.items():\n",
        "      exec('self.' + param + ' = ' + str(val))\n",
        "\n",
        "\t\t# Sets the seed if specified\n",
        "    if self.seed != None:\n",
        "\t\t\t# Check if our seed is valid first\n",
        "      assert(type(self.seed) == int)\n",
        "\n",
        "\t\t\t# Set the seed\n",
        "      torch.manual_seed(self.seed)\n",
        "      print(f\"Successfully set seed to {self.seed}\")\n",
        "\n",
        "  def _log_summary(self):\n",
        "\t\t# Calculate logging values. I use a few python shortcuts to calculate each value\n",
        "\t\t# without explaining since it's not too important to PPO; feel free to look it over,\n",
        "\t\t# and if you have any questions you can email me (look at bottom of README)\n",
        "    delta_t = self.logger['delta_t']\n",
        "    self.logger['delta_t'] = time.time_ns()\n",
        "    delta_t = (self.logger['delta_t'] - delta_t) / 1e9\n",
        "    delta_t = str(round(delta_t, 2))\n",
        "\n",
        "    t_so_far = self.logger['t_so_far']\n",
        "    i_so_far = self.logger['i_so_far']\n",
        "    avg_ep_lens = np.mean(self.logger['batch_lens'])\n",
        "    avg_ep_rews = np.mean([np.sum(ep_rews) for ep_rews in self.logger['batch_rews']])\n",
        "    avg_actor_loss = np.mean([losses.float().mean() for losses in self.logger['actor_losses']])\n",
        "\n",
        "\t\t# Round decimal places for more aesthetic logging messages\n",
        "    avg_ep_lens = str(round(avg_ep_lens, 2))\n",
        "    avg_ep_rews = str(round(avg_ep_rews, 2))\n",
        "    avg_actor_loss = str(round(avg_actor_loss, 5))\n",
        "\n",
        "\t\t# Print logging statements\n",
        "    print(flush=True)\n",
        "    print(f\"-------------------- Iteration #{i_so_far} --------------------\", flush=True)\n",
        "    print(f\"Average Episodic Length: {avg_ep_lens}\", flush=True)\n",
        "    print(f\"Average Episodic Return: {avg_ep_rews}\", flush=True)\n",
        "    print(f\"Average Loss: {avg_actor_loss}\", flush=True)\n",
        "    print(f\"Timesteps So Far: {t_so_far}\", flush=True)\n",
        "    print(f\"Iteration took: {delta_t} secs\", flush=True)\n",
        "    print(f\"------------------------------------------------------\", flush=True)\n",
        "    print(flush=True)\n",
        "\n",
        "\t\t# Reset batch-specific logging data\n",
        "    self.logger['batch_lens'] = []\n",
        "    self.logger['batch_rews'] = []\n",
        "    self.logger['actor_losses'] = []"
      ],
      "metadata": {
        "id": "9-bJ20YxayZv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pendulum-v1')\n",
        "model = PPO(policy_class=FeedForwardNN, env=env)\n",
        "model.learn(48000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4UnZJiDoQaM",
        "outputId": "2427ebb3-3b97-4799-f6d2-77bafd64f669"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning... Running 1600 timesteps per episode, 4800 timesteps per batch for a total of 48000 timesteps\n",
            "\n",
            "-------------------- Iteration #1 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1113.57\n",
            "Average Loss: 0.03262\n",
            "Timesteps So Far: 4800\n",
            "Iteration took: 9.17 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #2 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1195.56\n",
            "Average Loss: 0.00356\n",
            "Timesteps So Far: 9600\n",
            "Iteration took: 2.64 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #3 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1227.48\n",
            "Average Loss: -0.00062\n",
            "Timesteps So Far: 14400\n",
            "Iteration took: 3.07 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #4 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1212.7\n",
            "Average Loss: -0.00027\n",
            "Timesteps So Far: 19200\n",
            "Iteration took: 2.67 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #5 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1251.71\n",
            "Average Loss: -0.00017\n",
            "Timesteps So Far: 24000\n",
            "Iteration took: 2.64 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #6 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1176.22\n",
            "Average Loss: -0.00049\n",
            "Timesteps So Far: 28800\n",
            "Iteration took: 2.7 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #7 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1124.83\n",
            "Average Loss: 0.00014\n",
            "Timesteps So Far: 33600\n",
            "Iteration took: 2.97 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #8 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1150.41\n",
            "Average Loss: -0.00033\n",
            "Timesteps So Far: 38400\n",
            "Iteration took: 2.76 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #9 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1186.64\n",
            "Average Loss: -0.00036\n",
            "Timesteps So Far: 43200\n",
            "Iteration took: 2.63 secs\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Iteration #10 --------------------\n",
            "Average Episodic Length: 200.0\n",
            "Average Episodic Return: -1095.61\n",
            "Average Loss: -0.00033\n",
            "Timesteps So Far: 48000\n",
            "Iteration took: 2.64 secs\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}